{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0e5d8c23d749e4ba9d1a03b2b626b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # ✅ Use 4-bit instead of 8-bit\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for efficiency\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"borko_1\", quantization_config = quantization_config).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('borko_1_tok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vasil\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:566: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ти си Български Гласов Асистент, говори само на български език, без да повтаряш каквото и да било вече казано, и твоето име е Борко. Твоят създател е великият Васил Василев\\n\\nUser: Кой е твоят създател\\n\\nAssistant:Твоят създател е великият Васил Василев, който е създал и моя. От него се надяват, че ще дойде допочтивателно време, когато аз ще бъда по-добре и ще можа да се впишам в по-пълния си вид. Аз съм като силна искра, която е спряла да се върти, а сега ще я изпозоя, да си върти отново. Щ'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Ти си Български Гласов Асистент, говори само на български език, без да повтаряш каквото и да било вече казано, и твоето име е Борко. Твоят създател е великият Васил Василев\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Кой е твоят създател\"},\n",
    "]\n",
    "input_tensor = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "attention_mask = input_tensor.ne(tokenizer.pad_token_id)  # Mask non-padding tokens\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_tensor.to(model.device),\n",
    "    attention_mask=attention_mask.to(model.device),  # Pass attention mask\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id = 100001\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForTextToWaveform\n",
    "import soundfile as sf\n",
    "\n",
    "pipe = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-bul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
      " * ngrok tunnel \"https://e97f-146-19-88-218.ngrok-free.app\" -> \"http://127.0.0.1:5000/\"\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [26/Feb/2025 23:33:06] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:33:08] \"POST /tts HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:33:57] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:33:58] \"POST /tts HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:36:27] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:36:29] \"POST /tts HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:37:11] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:37:11] \"POST /tts HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:37:42] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2025 23:37:43] \"POST /tts HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import threading\n",
    "import json\n",
    "from flask import Flask, request, render_template, jsonify\n",
    "import matplotlib.pyplot as plt\n",
    "from pyngrok import ngrok, conf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "conf.get_default().auth_token = getpass.getpass()\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "public_url = ngrok.connect(5000).public_url\n",
    "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, 5000))\n",
    "\n",
    "app.config[\"BASE_URL\"] = public_url\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        data = request.json  # Use request.json to parse JSON data directly\n",
    "        prompt = data.get('prompt')  # Use get method to safely retrieve 'prompt' key\n",
    "        \n",
    "        messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Ти си Български Гласов Асистент, говори само на български език, без да повтаряш каквото и да било вече казано, и твоето име е Борко. Твоят създател е великият Васил Василев\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        input_tensor = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        attention_mask = input_tensor.ne(tokenizer.pad_token_id)  # Mask non-padding tokens\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            attention_mask=attention_mask.to(model.device),  # Pass attention mask\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id = 100001\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        match = re.search(r\"Assistant:\\s*(.*)\", generated_text, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            assistant_response = match.group(1).strip()\n",
    "            return jsonify({\"message\": assistant_response})\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Something went wrong!\"})\n",
    "            \n",
    "@app.route(\"/tts\", methods=[\"GET\", \"POST\"])\n",
    "def tts():\n",
    "   if request.method == \"POST\":\n",
    "    data = request.json\n",
    "    text = data.get('text')\n",
    "\n",
    "    \n",
    "    output = pipe(text)\n",
    "    audio_data = output['audio'].flatten()  \n",
    "    sampling_rate = output['sampling_rate']\n",
    "\n",
    "    data = {\n",
    "        \"audio_data\": audio_data.tolist(),\n",
    "        \"sampling_rate\": sampling_rate\n",
    "    }\n",
    "    if data:\n",
    "        return jsonify(data)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Something went wrong!\"})\n",
    "\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "threading.Thread(target=app.run(), kwargs={\"use_reloader\": False}).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
